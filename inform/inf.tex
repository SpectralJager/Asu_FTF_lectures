\documentclass[a4paper, 12pt]{article}

\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage { amsmath , amssymb , amsthm }
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\graphicspath{{img/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}


\title{Информатика}
\author{Шмаков И. А.}
\date{\today}

\begin{document}
\sffamily
\maketitle
\section*{Лекция 1}
\textbf{Информатика} -- это наука изучающая информационные аспекты процессов и системные аспекты информационных процессов.\\
Термин впрвые появился в 1957 году благодаря Карлу Штейнбуху. В 1962 Дрейфусом во Франции. Харкувич в 1962 в СССР.\\
\textbf{Информация} -- свединия или объект о чем-то
\textbf{Объем данных} -- кол-во байт, необходимых для их хранения в памяти электронного носителя. Бит -- базовая единица измерения кол-ва информации.\\
\textbf{Машинное слово} -- машино-зависящее и платформо-зависящее величина, измеряющаяся в битах или байтах.
\\ Перевод из одной системы счисления в другую:\\
$ 10_{10} \to N_{2} $, делим число на 2 и ее остаток пока не получим 1 и дальше делить не можем и записываем в обратном порядке остатки.\\
Двоичное представление:\\
10:ABCD\\
0:0000\\
1:0001\\
2:0010\\
  ...\\
8:1000\\
\underline{9:1001}\\
10:1010\\
11:1011\\
12:1100\\
13:1101\\
14:1110\\
15:1111\\

Для возвращения в 10ную систему счисления нужно возвести в степень($   1001111_2 = 1\cdot2^6 +1\cdot2^3+1\cdot2^2+1\cdot2^1+1\cdot2^0 = 79_{10}$)




\section*{Лекция 2 -- Информация, кодирование информации, код Шенона и различные кодировки}

\textbf{Данные} -- подающееся многократной интерпретации, предвтавление инфомрации в формализованном виде, пригодном для передачи, связи или обработки.\\

\textbf{Свойства:}\\
1. Объективность\\
2. Достоверность\\
3. Полнота -- минимальный набор, достаточный для принятия решений\\
4. Адекватность \\
5. Доступность\\
6. Актуальность(только вовремя полученная информация является полезной)\\
7. Ценность\\
8. Понятность(ясность)\\
9. Точность\\
10. Атрибутивные св.\\
11. Динамические св.\\
12. Практические св.\\


\textbf{Теория информации} -- раздел прикладной математики, относящиийся к измерению кол-ва информации, ее свойств и устанавливающий предельные соотношения для систем передачи данных.\\
\begin{mdframed}[backgroundcolor=blue!20] 
       
  
\textbf{Схема передачи информации}

Источник информации $\to$ кодер инсточника $\to$ кодер канала $\to$ модулятор $\to$ среда распространения $\to$ демодулятор $\to$ декодер канала $\to$ декодер источниа $\to$ получатель информации
\end{mdframed}

\textbf{Передача информации} -- это заблагавременно организованное техническое мероприятие, результатом которого становится воспроизведение информации, имеющейся в одном месте,в другое место.\\

Мнформационная энтропя -- мера неопределенности или непредсказуемости некоторой системы, в часности неопределенность появления какого-либо символа первичного алфавита.\\
Энтропия -- это количество информации, приходящейся на одно элементароное сообщение источника, вырабатывающейго ...\\

\begin{mdframed}[backgroundcolor=blue!20] 
       Формула Хартли:\\
       \[l = \log_2 (N) = n \log_2 (m)\]
       N - кол-во возможной информации\\
       m - кол-во букв в алфавите\\
       n - кол-ао букв в сообщении\\
       $ l $ - кол-во информации в битах\\
       это верно при равновероятном появлении символа\\
    \end{mdframed}



\begin{mdframed}[backgroundcolor=blue!20] 
       Информационная двоичная энтропия для независимых случайных событий:\\
       \[
       	H(x) = -\sum_{i=1}^n p(i) \log_2 p(i)
       \]
    \end{mdframed}




\end{document}